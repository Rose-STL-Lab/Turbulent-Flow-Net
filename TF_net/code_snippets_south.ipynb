{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9 tensor(0.8984)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Masking code\n",
    "import torch\n",
    "import numpy as np\n",
    "def mask_gen(epoch, tile_sz=4, image_sz=64, start=15, end=85, lower = 80, upper=100):\n",
    "    assert image_sz%tile_sz == 0    #code not tested for other cases\n",
    "    mask_ratio = 0.01*min(upper, max(lower, lower + ((upper-lower)*(epoch-start))/(end-start)))\n",
    "    iv,jv = [x.flatten() for x in np.meshgrid(np.arange(image_sz//tile_sz), np.arange(image_sz//tile_sz), indexing='ij')]\n",
    "    mask_i = np.random.choice(len(iv), int(mask_ratio*len(iv)), replace=False)\n",
    "\n",
    "    mask = torch.ones((image_sz,image_sz))\n",
    "    for idx in zip(mask_i):\n",
    "        i, j = iv[idx]*tile_sz, jv[idx]*tile_sz\n",
    "        mask[i:i+tile_sz,j:j+tile_sz] = 0\n",
    "    print(mask_ratio, (mask == 0).sum() / len(mask.flatten()))\n",
    "    return mask\n",
    "    \n",
    "mask_gen(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'positional_encodings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#Postiional encoding\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpositional_encodings\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch_encodings\u001b[39;00m \u001b[39mimport\u001b[39;00m PositionalEncoding1D\n\u001b[1;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_pos_emb\u001b[39m(tstep, xx_len, test_mode\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, w\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, h\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m):\n\u001b[1;32m      6\u001b[0m     inp_len \u001b[39m=\u001b[39m xx_len \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'positional_encodings'"
     ]
    }
   ],
   "source": [
    "#Postiional encoding\n",
    "import torch\n",
    "from positional_encodings.torch_encodings import PositionalEncoding1D\n",
    "\n",
    "def get_pos_emb(tstep, xx_len, test_mode=1, w=64, h=64):\n",
    "    inp_len = xx_len // 2\n",
    "\n",
    "    def single_channel(tstep, inp_len, dim, b_size=1):\n",
    "        # b_size is irrelavant, the value will be same for each batch dimension\n",
    "        p_enc_1d_model = PositionalEncoding1D(dim)\n",
    "        emb_full =  p_enc_1d_model(torch.rand(b_size, tstep + inp_len, dim))\n",
    "        return emb_full[:,-inp_len:]\n",
    "\n",
    "    single_ch_emb = single_channel(tstep, inp_len, h*w, 1)\n",
    "    # we want concatenation to be alternate: https://stackoverflow.com/questions/61026393/pytorch-concatenate-rows-in-alternate-order\n",
    "    # After operation, emb[0,i] == emb[0,i+1] where i is even\n",
    "    emb = torch.cat((single_ch_emb, single_ch_emb), axis=-1).reshape((1, -1, h, w))  \n",
    "    if test_mode > 1:\n",
    "        emb = [emb for _ in range(test_mode)]\n",
    "        emb = torch.cat(emb, dim=-1)\n",
    "    return emb\n",
    "\n",
    "emb = get_pos_emb(2, 62, test_mode=7)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
